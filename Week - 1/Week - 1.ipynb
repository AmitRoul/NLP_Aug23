{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700344c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"english\")\n",
    "ss.stem(\"playing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9d3c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "wnl.lemmatize(\"running\", pos='v')  # to lemmatize 'running' as a verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40fc34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize(\"running\", pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ed6c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happier: happy\n"
     ]
    }
   ],
   "source": [
    "#Loading the NLTK package for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#creating the Object for Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Printing the lemma of word happier\n",
    "print(\"happier:\", lemmatizer.lemmatize(\"happier\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da413ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text in lowercase : python is a programming language that is interpreted and high-level language\n"
     ]
    }
   ],
   "source": [
    "#Identify the user input\n",
    "user_input = \"Python is a Programming Language that is Interpreted and High-Level language\"\n",
    "\n",
    "#Print the sentence by converting the text from uppercase to lowercase\n",
    "print(\"Text in lowercase :\", user_input.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274a8c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove numbers : Team A has  batsman and  bowlers, while team b has  batsman and  bowlers\n"
     ]
    }
   ],
   "source": [
    "#Loading the regex package to find number\n",
    "import re\n",
    "\n",
    "#identify user input\n",
    "input_str = \"Team A has 690707 batsman and 5 bowlers, while team b has 5 batsman and 6 bowlers\"\n",
    "\n",
    "#remove numbers by using regex\n",
    "output = re.sub(r\"\\d+\", \"\", input_str)\n",
    "\n",
    "#print the sentence after removal of numbers\n",
    "print(\"remove numbers :\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d7aaeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     pythonis programming    language \\t\\n\\r\\tHello \\t'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the regex and string package\n",
    "import re\n",
    "\n",
    "#define input from user\n",
    "input_str = '     pythonis programming    language \\t\\n\\r\\tHello \\t'\n",
    "input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eb541a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove spaces using regex :pythonisprogramminglanguageHello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the sentence after removing the spaces\n",
    "print('Remove spaces using regex :', re.sub(r\"\\s+\", \"\", input_str),\"\\n\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e18cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove landing spaces using regex :pythonis programming    language \t\n",
      "\r",
      "\tHello \t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the sentence after removing the landing spaces\n",
    "print('Remove landing spaces using regex :', re.sub(r\"^\\s+\", \"\", input_str),\"\\n\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6e19ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove trailing spaces using regex :     pythonis programming    language \t\n",
      "\r",
      "\tHello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the sentence after removing the trailing spaces\n",
    "print('Remove trailing spaces using regex :', re.sub(r\"\\s+$\", \"\", input_str),\"\\n\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc5ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove landing spaces using regex :pythonis programming    language \t\n",
      "\r",
      "\tHello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the sentence after removing the leading and trailing spaces\n",
    "print('Remove landing spaces using regex :', re.sub(r\"^\\s+|\\s+$\", \"\", input_str),\"\\n\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6481f",
   "metadata": {},
   "source": [
    "## Creating a Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e91b741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.', 'the', 'dog', 'barked.']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumped over the lazy dog . The dog barked.\"\n",
    "\n",
    "# Tokenize the text (split by spaces for simplicity, but in practice, you might use a library like NLTK or spaCy)\n",
    "tokens = text.lower().split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99014b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 3,\n",
       "         'quick': 1,\n",
       "         'brown': 1,\n",
       "         'fox': 1,\n",
       "         'jumped': 1,\n",
       "         'over': 1,\n",
       "         'lazy': 1,\n",
       "         'dog': 2,\n",
       "         '.': 1,\n",
       "         'barked.': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vocabulary with word frequencies\n",
    "vocab = Counter(tokens)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c88d2479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('the', 3), ('quick', 1), ('brown', 1), ('fox', 1), ('jumped', 1), ('over', 1), ('lazy', 1), ('dog', 2), ('.', 1), ('barked.', 1)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.items()  # IS A ITERABLE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "384e70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 3)\n",
      "('quick', 1)\n",
      "('brown', 1)\n",
      "('fox', 1)\n",
      "('jumped', 1)\n",
      "('over', 1)\n",
      "('lazy', 1)\n",
      "('dog', 2)\n",
      "('.', 1)\n",
      "('barked.', 1)\n"
     ]
    }
   ],
   "source": [
    "# Display the vocabulary\n",
    "for item in vocab.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37ea7a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the': 3\n",
      "'quick': 1\n",
      "'brown': 1\n",
      "'fox': 1\n",
      "'jumped': 1\n",
      "'over': 1\n",
      "'lazy': 1\n",
      "'dog': 2\n",
      "'.': 1\n",
      "'barked.': 1\n"
     ]
    }
   ],
   "source": [
    "# Display the vocabulary\n",
    "for word, freq in vocab.items():  # unzipping the tuples by providing 2 iterator variables\n",
    "    print(f\"'{word}': {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc7857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11e89580",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64d3faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the stopwords package\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Load the word tokenizer package\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84cf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the user input\n",
    "input_str = \"Stop words are the words that are filtered before and after processing of text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1880163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")  # python list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39ba7c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))  # number of stopwords in nltk libreary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb893da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crete object for stopwords\n",
    "nltk_stopwords = set(stopwords.words(\"english\"))\n",
    "type(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1290f73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a6c2929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hereupon', 'becomes', 'about', 'give', 'back', 'latterly', 'any', 'out', 'beforehand', 'almost', 'nevertheless', 'together', 'hundred', 'without', 'nothing', 'again', 'take', 'etc', 'mine', 'side', 'mill', 'from', 'amoungst', 'thence', 'mostly', 'whereupon', 'whereby', 'on', 'next', 'each', 'latter', 'go', 'upon', 'because', 'whether', 'with', 'his', 'that', 'full', 'someone', 'first', 'hasnt', 'bill', 'are', 'somewhere', 'con', 'anywhere', 'very', 'what', 'however', 'few', 'must', 'their', 'by', 'he', 'can', 'will', 'eight', 'hers', 'now', 'yours', 'de', 'interest', 'neither', 'former', 'detail', 'i', 'who', 'five', 'among', 'up', 'than', 'whereas', 'may', 'too', 'our', 'some', 'there', 'due', 'amongst', 'around', 'be', 'as', 'whither', 'amount', 'otherwise', 'whose', 'before', 'call', 'several', 'how', 'show', 'everywhere', 'also', 'never', 'if', 'front', 'your', 'no', 'therein', 'could', 'whoever', 'these', 'anyhow', 'nine', 'fill', 'hence', 'moreover', 'own', 'one', 'formerly', 'other', 'itself', 'ourselves', 'all', 'another', 'whole', 'of', 'besides', 'system', 'yet', 'name', 'eg', 'so', 'seem', 'throughout', 'therefore', 'except', 'both', 'and', 'her', 'which', 'others', 'anyway', 'they', 'namely', 'yourself', 'elsewhere', 'well', 'its', 'himself', 'indeed', 'noone', 'serious', 'else', 'thereafter', 'done', 'should', 'afterwards', 'into', 'to', 'anything', 'more', 'via', 'see', 'or', 'perhaps', 'me', 'anyone', 'is', 'cant', 'couldnt', 'wherever', 'against', 'once', 'please', 'whereafter', 'became', 'ie', 'between', 'whence', 'put', 'herein', 'fifteen', 'my', 'hereafter', 'thick', 'made', 'part', 'such', 'above', 'keep', 'most', 'often', 'everyone', 'beside', 'move', 'last', 'you', 'until', 'becoming', 'seems', 'behind', 'forty', 'thin', 'across', 'meanwhile', 'after', 'sometime', 'would', 'has', 'least', 'ours', 'whenever', 'only', 'where', 'eleven', 'cannot', 'bottom', 'twelve', 'whom', 'empty', 'co', 'do', 'become', 'had', 'always', 'ever', 'fire', 'third', 'seemed', 'hereby', 'although', 'since', 'an', 'every', 'the', 'same', 'seeming', 'per', 'find', 'them', 'less', 'while', 'during', 'down', 'then', 'thereupon', 'describe', 'nor', 'still', 'yourselves', 'over', 'thereby', 'this', 'for', 'when', 'beyond', 'even', 'inc', 'themselves', 'him', 'fifty', 'under', 'none', 'found', 'might', 'either', 'enough', 'being', 'onto', 'four', 'along', 'us', 'myself', 'am', 'was', 're', 'ltd', 'un', 'everything', 'though', 'six', 'were', 'through', 'those', 'thru', 'off', 'already', 'alone', 'thus', 'many', 'further', 'in', 'it', 'here', 'sometimes', 'been', 'a', 'towards', 'why', 'top', 'nowhere', 'we', 'rather', 'within', 'at', 'below', 'somehow', 'sixty', 'wherein', 'cry', 'two', 'three', 'much', 'toward', 'something', 'twenty', 'she', 'not', 'but', 'whatever', 'get', 'have', 'sincere', 'herself', 'nobody', 'ten'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Convert the frozen set to a list\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Print the first few stopwords\n",
    "print(sklearn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4d6ad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sklearn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2ed2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8dd04c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hereupon', 'becomes', 'about', 'give', 'back', 'latterly', 'any', 'out', 'beforehand', 'almost', 'nevertheless', 'together', 'hundred', 'without', 'doing', 'nothing', 'again', 'take', 'mine', 'side', '’d', 'from', 'thence', 'mostly', 'whereupon', 'whereby', 'on', 'next', 'each', 'latter', 'go', 'upon', 'because', 'whether', 'with', 'his', 'that', \"'re\", 'full', 'someone', 'first', 'are', 'somewhere', 'anywhere', 'very', 'what', 'however', 'few', 'must', 'their', \"n't\", 'by', 'he', 'can', 'will', 'various', 'eight', 'hers', 'now', 'yours', '’m', 'neither', 'n‘t', 'former', 'i', 'who', \"'s\", 'five', 'among', 'up', 'than', 'whereas', 'may', 'too', '’re', 'our', '‘ll', 'some', 'there', 'due', 'amongst', 'around', 'be', 'as', 'whither', 'amount', 'used', 'otherwise', 'whose', 'before', 'call', 'several', 'how', 'make', 'show', 'everywhere', 'also', 'never', 'if', 'front', 'your', 'no', 'therein', 'could', 'whoever', 'these', 'anyhow', '’ll', 'nine', 'hence', '‘m', 'moreover', 'did', 'own', 'one', 'formerly', 'other', 'itself', 'ourselves', 'all', 'another', 'whole', 'of', 'does', 'besides', 'yet', 'name', '‘d', \"'ve\", 'so', 'seem', 'throughout', 'therefore', 'except', 'both', 'and', 'her', 'which', 'others', 'anyway', 'they', 'namely', 'yourself', 'elsewhere', 'well', 'its', 'himself', 'indeed', 'noone', 'serious', 'else', 'thereafter', 'done', 'should', 'afterwards', 'into', 'to', 'anything', 'more', '‘ve', 'via', 'see', '‘re', 'unless', 'or', 'perhaps', 'me', 'anyone', 'is', 'wherever', 'against', 'once', 'please', 'whereafter', 'became', 'quite', 'regarding', 'between', 'whence', \"'ll\", 'put', 'herein', 'fifteen', 'my', 'hereafter', 'made', 'part', 'such', 'above', 'keep', 'most', 'often', 'everyone', 'beside', 'move', 'last', 'you', 'until', 'becoming', 'seems', \"'d\", 'behind', 'forty', 'across', 'meanwhile', 'after', '’ve', 'sometime', 'ca', 'would', 'has', 'least', 'ours', 'whenever', '‘s', 'only', 'where', 'eleven', 'cannot', 'bottom', 'twelve', 'whom', 'empty', 'do', 'become', 'had', 'always', 'ever', 'third', 'seemed', 'hereby', 'although', 'since', 'an', 'every', 'the', 'same', 'seeming', 'per', 'them', 'less', 'while', 'during', 'down', 'then', 'thereupon', 'nor', 'still', 'yourselves', 'over', 'thereby', 'this', '’s', 'for', 'when', 'beyond', 'even', 'themselves', 'him', 'fifty', 'under', 'none', 'might', 'either', 'enough', 'being', 'onto', 'four', 'along', 'us', 'just', 'myself', 'am', 'was', 're', 'everything', 'though', 'six', 'were', \"'m\", 'through', 'those', 'n’t', 'thru', 'off', 'already', 'alone', 'thus', 'many', 'really', 'further', 'in', 'it', 'here', 'sometimes', 'been', 'a', 'towards', 'why', 'top', 'within', 'we', 'using', 'nowhere', 'rather', 'at', 'below', 'somehow', 'sixty', 'say', 'wherein', 'two', 'three', 'much', 'toward', 'something', 'twenty', 'she', 'not', 'but', 'whatever', 'get', 'have', 'herself', 'nobody', 'ten'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the list of stopwords\n",
    "spacy_stopwords = set(nlp.Defaults.stop_words)\n",
    "\n",
    "# Print the stopwords\n",
    "print(spacy_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70f0af50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc9633aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'do',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'rather',\n",
       " 're',\n",
       " 'same',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_stopwords.intersection(spacy_stopwords) # common stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7401335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sklearn_stopwords.intersection(spacy_stopwords) )  # number of common stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "256cd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_scikit_stopwords = sklearn_stopwords.union(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa61a766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_scikit_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfa1a618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_scikit_nltk_stopwords = spacy_scikit_stopwords.union(nltk_stopwords)\n",
    "len(spacy_scikit_nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5bf5794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actor', 'cast', 'director', 'film', 'movie', 'song', 'story'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_stopwords = {'movie', 'film', 'actor', 'director', 'song', 'cast', 'story'}\n",
    "domain_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bf91635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_stopwords = spacy_scikit_nltk_stopwords.union(domain_stopwords)\n",
    "len(combined_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62773f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stop',\n",
       " 'words',\n",
       " 'are',\n",
       " 'the',\n",
       " 'words',\n",
       " 'that',\n",
       " 'are',\n",
       " 'filtered',\n",
       " 'before',\n",
       " 'and',\n",
       " 'after',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'text',\n",
       " '.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert word into tokens\n",
    "tokens = word_tokenize(input_str)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ed8a971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stop', 'words', 'words', 'filtered', 'processing', 'text', '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords from the list of tokens\n",
    "output = [token for token in tokens if not token in stop_word]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42fbf483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove stopwords : ['Stop', 'words', 'words', 'filtered', 'processing', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "#remove the stopwords and print the sentence\n",
    "print(\"remove stopwords :\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d6ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "074e6716",
   "metadata": {},
   "source": [
    "## Sentence-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6fee56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the package for tokenizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feba179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining user input\n",
    "text = \"\"\"This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode the pad token.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8fb2282a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\\n\\nAs a convention, \"0\" does not stand for a specific word, but instead is used to encode the pad token.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c766f031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative).',\n",
       " 'Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers).',\n",
       " 'For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data.',\n",
       " 'This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".',\n",
       " 'As a convention, \"0\" does not stand for a specific word, but instead is used to encode the pad token.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the tokens of sentence.\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e271d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(text))   # we have 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b474204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW : Apply stop word removal on the above list of tokenized sentences.\n",
    "# Output will be another list of tokenized sentences, with the stopwords removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
